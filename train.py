# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ BiLSTM å’Œ BiGRU å¯¹ data_model_141.xlsx è¿›è¡Œ
æ„ŸæŸ“(0) vs è„“æ¯’ç—‡(1) äºŒåˆ†ç±»ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å’Œé›†æˆï¼Œ
åŒæ—¶å¯¼å‡ºæµ‹è¯•é›†ä¸Šæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡ã€‚
"""

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    confusion_matrix,
    classification_report,
)
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ==================== å…¨å±€é…ç½® ====================

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å½“å‰ä½¿ç”¨è®¾å¤‡: {DEVICE}")

DATA_PATH = r"D:\PycharmProjects\åˆ¤å®šè„“æ¯’ç—‡çš„è›‹ç™½æ£€æµ‹\data_model_141.xlsx"
LABEL_COL = "sepsis_group"
ID_COL = "eid"   # ç”¨ä½œIDï¼Œä¸ä½œä¸ºç‰¹å¾

RANDOM_SEED = 42
BATCH_SIZE = 128
EPOCHS = 80
LR = 1e-3
WEIGHT_DECAY = 1e-4
EARLY_STOP_PATIENCE = 10   # AUC è¿ç»­å¤šå°‘ epoch ä¸æå‡å°±æ—©åœ

BASE_DIR = os.path.dirname(DATA_PATH)
CLASSIC_PROB_PATH = os.path.join(BASE_DIR, "classic_models_test_probs_141.xlsx")
DEEP_PROB_PATH = os.path.join(BASE_DIR, "deep_models_test_probs_141.xlsx")
ALL_PROB_PATH = os.path.join(BASE_DIR, "all_models_test_probs_141.xlsx")


# ==================== å·¥å…·å‡½æ•°ï¼šè®¾å®šéšæœºç§å­ ====================

def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


seed_everything(RANDOM_SEED)


# ==================== æ•°æ®åŠ è½½ä¸åˆ’åˆ† ====================

def load_and_split_data(
    path,
    label_col=LABEL_COL,
    id_col=ID_COL,
    test_size=0.2,
    val_size=0.2,
    random_state=RANDOM_SEED,
):
    # 1) è¯» Excel
    df = pd.read_excel(path)

    print(f"æ•°æ®å½¢çŠ¶ï¼š {df.shape}")
    print("åˆ—åé¢„è§ˆï¼š", list(df.columns[:10]), " ...")

    # 2) æ ‡ç­¾åˆ—æ£€æŸ¥
    if label_col not in df.columns:
        cand = [c for c in df.columns if "sepsis" in c.lower()]
        raise ValueError(f"æ‰¾ä¸åˆ°æ ‡ç­¾åˆ— {label_col}ï¼Œå€™é€‰åˆ—ï¼š{cand}")

    print(f"âœ… ä½¿ç”¨çš„æ ‡ç­¾åˆ—ï¼š {label_col}")

    # 3) ç‰¹å¾ & æ ‡ç­¾ & ID
    feature_cols = [c for c in df.columns if c not in [label_col, id_col]]
    X = df[feature_cols].values.astype(np.float32)
    y = df[label_col].values.astype(int)
    ids = df[id_col].values

    print(f"ç‰¹å¾æ•°é‡ï¼š {X.shape[1]}")
    print("æ ‡ç­¾åˆ†å¸ƒï¼š")
    print(pd.Series(y).value_counts())

    # 4) è®­ç»ƒ / æµ‹è¯• åˆ’åˆ†ï¼ˆä¿æŒä¸ä¼ ç»Ÿæ¨¡å‹è„šæœ¬ä¸€è‡´ï¼šåŒæ ·çš„ random_state å’Œ test_sizeï¼‰
    X_trainval, X_test, y_trainval, y_test, id_trainval, id_test = train_test_split(
        X, y, ids,
        test_size=test_size,
        random_state=random_state,
        stratify=y,
    )

    # å†ä» trainval ä¸­åˆ‡å‡ºéªŒè¯é›†
    val_ratio = val_size / (1.0 - test_size)
    X_train, X_val, y_train, y_val, id_train, id_val = train_test_split(
        X_trainval, y_trainval, id_trainval,
        test_size=val_ratio,
        random_state=random_state,
        stratify=y_trainval,
    )

    print(
        f"è®­ç»ƒé›†å¤§å°ï¼š ({X_train.shape[0]}, {X_train.shape[1]})   "
        f"éªŒè¯é›†å¤§å°ï¼š ({X_val.shape[0]}, {X_val.shape[1]})   "
        f"æµ‹è¯•é›†å¤§å°ï¼š ({X_test.shape[0]}, {X_test.shape[1]})"
    )

    # 5) è®¡ç®— pos_weight
    num_pos = (y_train == 1).sum()
    num_neg = (y_train == 0).sum()
    pos_weight_value = num_neg / max(num_pos, 1)
    print(f"è®­ç»ƒé›†ä¸­é˜³æ€§={num_pos}, é˜´æ€§={num_neg}, pos_weight={pos_weight_value:.2f}")

    # 6) ç¼ºå¤±å€¼å¡«è¡¥ + æ ‡å‡†åŒ–ï¼ˆåªç”¨è®­ç»ƒé›†æ‹Ÿåˆï¼‰
    imputer = SimpleImputer(strategy="mean")
    scaler = StandardScaler()

    X_train = imputer.fit_transform(X_train)
    X_train = scaler.fit_transform(X_train)

    X_val = imputer.transform(X_val)
    X_val = scaler.transform(X_val)

    X_test = imputer.transform(X_test)
    X_test = scaler.transform(X_test)

    # è½¬æˆ float32
    X_train = X_train.astype(np.float32)
    X_val = X_val.astype(np.float32)
    X_test = X_test.astype(np.float32)

    pos_weight_tensor = torch.tensor(pos_weight_value, dtype=torch.float32, device=DEVICE)

    return (
        X_train,
        y_train,
        X_val,
        y_val,
        X_test,
        y_test,
        pos_weight_tensor,
        id_test,   # æŠŠæµ‹è¯•é›†çš„ eid ä¸€èµ·è¿”å›
    )


# ==================== Dataset å®šä¹‰ ====================

class ProteinDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)  # (N, 141)
        self.y = torch.tensor(y, dtype=torch.float32)  # (N,)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        seq = self.X[idx]        # (141,)
        label = self.y[idx]      # æ ‡é‡
        return seq, label


# ==================== æ¨¡å‹å®šä¹‰ï¼šBiLSTM å’Œ BiGRU ====================

class BiLSTM(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        x = x.unsqueeze(-1)  # (batch, seq_len, 1)
        out, _ = self.lstm(x)  # out: (batch, seq_len, 2*hidden)
        last = out[:, -1, :]   # (batch, 2*hidden)
        logits = self.fc(last).squeeze(-1)  # (batch,)
        return logits


class BiGRU(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3):
        super().__init__()
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        x = x.unsqueeze(-1)  # (batch, seq_len, 1)
        out, _ = self.gru(x)  # (batch, seq_len, 2*hidden)
        last = out[:, -1, :]
        logits = self.fc(last).squeeze(-1)
        return logits


# ==================== è¯„ä¼°å‡½æ•°ï¼ˆè®­ç»ƒ & æµ‹è¯•å…¬ç”¨ï¼‰ ====================

def evaluate(model, dataloader, name="Model", threshold=0.5, verbose=True):
    model.eval()
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)

            logits = model(X_batch)  # (batch,)
            probs = torch.sigmoid(logits)

            all_probs.append(probs.detach().cpu().numpy())
            all_labels.append(y_batch.detach().cpu().numpy())

    y_prob = np.concatenate(all_probs, axis=0)
    y_true = np.concatenate(all_labels, axis=0)

    # ä¿é™©ï¼šæŠŠ NaN / Inf æ¸…æ‰ï¼Œé¿å… roc_auc_score æŠ¥é”™
    y_prob = np.nan_to_num(y_prob, nan=0.5, posinf=1.0, neginf=0.0)

    # AUC éœ€è¦æ­£è´Ÿç±»éƒ½å­˜åœ¨ï¼Œå¦åˆ™æŠ¥é”™
    try:
        if len(np.unique(y_true)) < 2:
            val_auc = 0.5
        else:
            val_auc = roc_auc_score(y_true, y_prob)
    except Exception as e:
        print(f"[{name}] è®¡ç®— AUC å‡ºé”™ï¼š{e}ï¼Œå°† AUC ç½®ä¸º 0.5")
        val_auc = 0.5

    y_pred = (y_prob >= threshold).astype(int)
    acc = accuracy_score(y_true, y_pred)

    if verbose:
        print(f"[{name}] AUC = {val_auc:.4f}, ACC = {acc:.4f}")
        cm = confusion_matrix(y_true, y_pred)
        print("æ··æ·†çŸ©é˜µï¼š")
        print(cm)
        print("åˆ†ç±»æŠ¥å‘Šï¼š")
        print(classification_report(y_true, y_pred, digits=4))

    return val_auc, acc, y_true, y_prob, y_pred


# ==================== è®­ç»ƒå‡½æ•° ====================

def train_model(
    model,
    train_loader,
    val_loader,
    pos_weight,
    num_epochs=EPOCHS,
    model_name="Model",
):
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    best_auc = -1.0
    best_state_dict = None
    no_improve_epochs = 0

    for epoch in range(1, num_epochs + 1):
        model.train()
        running_loss = 0.0

        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)

            optimizer.zero_grad()
            logits = model(X_batch)  # (batch,)
            loss = criterion(logits, y_batch)

            if torch.isnan(loss):
                print(f"[{model_name}] ç¬¬ {epoch} ä¸ª epoch å‡ºç° NaN lossï¼Œè·³è¿‡è¯¥ batch")
                continue

            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            running_loss += loss.item() * X_batch.size(0)

        avg_loss = running_loss / len(train_loader.dataset)

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°ï¼ˆä¸æ‰“å°æ··æ·†çŸ©é˜µï¼Œé¿å…å¤ªå¤šè¾“å‡ºï¼‰
        val_auc, val_acc, _, _, _ = evaluate(
            model, val_loader, name=model_name, verbose=False
        )

        print(
            f"[{model_name}] Epoch [{epoch}/{num_epochs}] "
            f"Train Loss = {avg_loss:.4f} | Val AUC = {val_auc:.4f} | Val ACC = {val_acc:.4f}"
        )

        # æ—©åœé€»è¾‘ï¼šVal AUC æå‡å°±ä¿å­˜
        if val_auc > best_auc:
            best_auc = val_auc
            best_state_dict = model.state_dict()
            no_improve_epochs = 0
            print(f"  ğŸ”¥ [{model_name}] Val AUC æå‡ä¸º {val_auc:.4f}ï¼Œä¿å­˜å½“å‰æ¨¡å‹")
        else:
            no_improve_epochs += 1
            if no_improve_epochs >= EARLY_STOP_PATIENCE:
                print(
                    f"  â—[{model_name}] Val AUC è¿ç»­ {EARLY_STOP_PATIENCE} ä¸ª epoch æœªæå‡ï¼Œæå‰åœæ­¢"
                )
                break

    if best_state_dict is not None:
        model.load_state_dict(best_state_dict)

    print(f"\n================= {model_name} åœ¨éªŒè¯é›†ä¸Šçš„æœ€ç»ˆè¡¨ç° =================")
    evaluate(model, val_loader, name=model_name + " Final Val", verbose=True)

    return model, best_auc


# ==================== ä¸»å‡½æ•° ====================

def main():
    # 1) åŠ è½½æ•°æ®
    (
        X_train,
        y_train,
        X_val,
        y_val,
        X_test,
        y_test,
        pos_weight,
        id_test,
    ) = load_and_split_data(DATA_PATH)

    # 2) æ„é€  Dataset / DataLoader
    train_dataset = ProteinDataset(X_train, y_train)
    val_dataset = ProteinDataset(X_val, y_val)
    test_dataset = ProteinDataset(X_test, y_test)

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        drop_last=False,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        drop_last=False,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        drop_last=False,
    )

    # 3) BiLSTM
    bilstm = BiLSTM(input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3).to(DEVICE)
    print(bilstm)
    print("\n================= å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š BiLSTM =================")
    bilstm, best_auc_lstm = train_model(
        bilstm, train_loader, val_loader, pos_weight, num_epochs=EPOCHS, model_name="BiLSTM"
    )

    print("\n================= BiLSTM åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° =================")
    _, _, y_test_true, y_test_prob_lstm, _ = evaluate(
        bilstm, test_loader, name="BiLSTM Test", verbose=True
    )

    # 4) BiGRU
    bigru = BiGRU(input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3).to(DEVICE)
    print(bigru)
    print("\n================= å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š BiGRU =================")
    bigru, best_auc_gru = train_model(
        bigru, train_loader, val_loader, pos_weight, num_epochs=EPOCHS, model_name="BiGRU"
    )

    print("\n================= BiGRU åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° =================")
    _, _, _, y_test_prob_gru, _ = evaluate(
        bigru, test_loader, name="BiGRU Test", verbose=True
    )

    # 5) ç®€å•é›†æˆï¼šBiLSTM + BiGRU æ¦‚ç‡å¹³å‡
    print("\n================= å¤šæ¨¡å‹é›†æˆï¼ˆBiLSTM + BiGRUï¼Œç­‰æƒæ¦‚ç‡å¹³å‡ï¼‰ =================")
    ens_probs_eq = (y_test_prob_lstm + y_test_prob_gru) / 2.0
    ens_probs_eq = np.nan_to_num(ens_probs_eq, nan=0.5, posinf=1.0, neginf=0.0)

    try:
        if len(np.unique(y_test_true)) < 2:
            ens_auc = 0.5
        else:
            ens_auc = roc_auc_score(y_test_true, ens_probs_eq)
    except Exception as e:
        print(f"[Ensemble] è®¡ç®— AUC å‡ºé”™ï¼š{e}ï¼Œå°† AUC ç½®ä¸º 0.5")
        ens_auc = 0.5

    ens_pred = (ens_probs_eq >= 0.5).astype(int)
    ens_acc = accuracy_score(y_test_true, ens_pred)

    print(f"Equal-Weighted Ensemble Test AUC = {ens_auc:.4f}, ACC = {ens_acc:.4f}")
    cm = confusion_matrix(y_test_true, ens_pred)
    print("ç­‰æƒ Ensemble æ··æ·†çŸ©é˜µï¼š")
    print(cm)
    print("ç­‰æƒ Ensemble åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test_true, ens_pred, digits=4))

    print("\n=====================================================================\n")

    # 6) å¯¼å‡ºæ·±åº¦æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹æ¦‚ç‡
    df_deep = pd.DataFrame({
        "eid": id_test,
        "label": y_test_true.astype(int),
        "prob_BiLSTM": y_test_prob_lstm.ravel(),
        "prob_BiGRU": y_test_prob_gru.ravel(),
        "prob_Ensemble": ens_probs_eq.ravel(),
    })

    df_deep = df_deep.sort_values(by="eid").reset_index(drop=True)
    df_deep.to_excel(DEEP_PROB_PATH, index=False)
    print(f"âœ… æ·±åº¦æ¨¡å‹é¢„æµ‹æ¦‚ç‡å·²ä¿å­˜åˆ°: {DEEP_PROB_PATH}")

    # 7) å¦‚æœ‰ä¼ ç»Ÿæ¨¡å‹æ¦‚ç‡æ–‡ä»¶ï¼Œåˆ™åˆå¹¶ç”Ÿæˆæ€»è¡¨
    if os.path.exists(CLASSIC_PROB_PATH):
        df_classic = pd.read_excel(CLASSIC_PROB_PATH)
        df_all = pd.merge(df_classic, df_deep, on=["eid", "label"], how="inner")
        df_all.to_excel(ALL_PROB_PATH, index=False)
        print(f"âœ… ä¸ä¼ ç»Ÿæ¨¡å‹æ¦‚ç‡å·²åˆå¹¶ï¼Œä¿å­˜åˆ°: {ALL_PROB_PATH}")
        print(f"åˆå¹¶åæ ·æœ¬æ•°: {df_all.shape[0]}")
    else:
        print(f"âš  æœªæ‰¾åˆ°ä¼ ç»Ÿæ¨¡å‹æ¦‚ç‡æ–‡ä»¶: {CLASSIC_PROB_PATH}ï¼Œä»…ä¿å­˜äº†æ·±åº¦æ¨¡å‹æ¦‚ç‡ã€‚")


if __name__ == "__main__":
    main()
