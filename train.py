# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ BiLSTM å’Œ BiGRU å¯¹ data_ukb.xlsx è¿›è¡Œ
æ„ŸæŸ“(0) vs è„“æ¯’ç—‡(1) äºŒåˆ†ç±»ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å’Œé›†æˆã€‚

æœ¬è„šæœ¬åŒ…æ‹¬ï¼š
- æ•°æ®åŠ è½½ä¸åˆ’åˆ†ï¼ˆtrain/val/testï¼‰
- ç®€å•çš„ç¼ºå¤±å€¼å¡«è¡¥ä¸æ ‡å‡†åŒ–
- BiLSTM / BiGRU æ¨¡å‹ç»“æ„å®šä¹‰
- è®­ç»ƒä¸æ—©åœ
- åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šçš„æ€§èƒ½è¯„ä¼°
"""

import os
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    confusion_matrix,
    classification_report,
)
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# ==================== å…¨å±€é…ç½® ====================

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"å½“å‰ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# æ•°æ®è·¯å¾„ä¸åˆ—åï¼ˆå¼€æºæ—¶è¯·åœ¨ README ä¸­è¯´æ˜æ•°æ®æ ¼å¼ï¼‰
DATA_PATH = r"D:\PycharmProjects\åˆ¤å®šè„“æ¯’ç—‡çš„è›‹ç™½æ£€æµ‹\data_ukb.xlsx"
LABEL_COL = "sepsis_group"   # æ ‡ç­¾åˆ—ï¼š0=æ„ŸæŸ“, 1=è„“æ¯’ç—‡
ID_COL = "eid"               # ç”¨ä½œIDï¼Œä¸ä½œä¸ºç‰¹å¾ï¼ˆå¦‚æœæ²¡æœ‰å¯ä»¥åˆ æ‰æ­¤åˆ—åï¼‰

RANDOM_SEED = 42
BATCH_SIZE = 128
EPOCHS = 80
LR = 1e-3
WEIGHT_DECAY = 1e-4
EARLY_STOP_PATIENCE = 10     # éªŒè¯é›† AUC è¿ç»­å¤šå°‘ epoch ä¸æå‡å°±æ—©åœ


# ==================== å·¥å…·å‡½æ•°ï¼šè®¾å®šéšæœºç§å­ ====================

def seed_everything(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


seed_everything(RANDOM_SEED)


# ==================== æ•°æ®åŠ è½½ä¸åˆ’åˆ† ====================

def load_and_split_data(
    path,
    label_col=LABEL_COL,
    id_col=ID_COL,
    test_size=0.2,
    val_size=0.2,
    random_state=RANDOM_SEED,
):
    """è¯»å– Excelï¼Œå®Œæˆ train/val/test åˆ’åˆ†ï¼Œå¹¶åšç¼ºå¤±å€¼å¡«è¡¥å’Œæ ‡å‡†åŒ–ã€‚"""
    df = pd.read_excel(path)

    print(f"æ•°æ®å½¢çŠ¶ï¼š {df.shape}")
    print("åˆ—åé¢„è§ˆï¼š", list(df.columns[:10]), " ...")

    # æ ‡ç­¾åˆ—æ£€æŸ¥
    if label_col not in df.columns:
        cand = [c for c in df.columns if "sepsis" in c.lower()]
        raise ValueError(f"æ‰¾ä¸åˆ°æ ‡ç­¾åˆ— {label_col}ï¼Œå€™é€‰åˆ—ï¼š{cand}")
    print(f"âœ… ä½¿ç”¨çš„æ ‡ç­¾åˆ—ï¼š {label_col}")

    # ç‰¹å¾åˆ—ï¼šå»æ‰æ ‡ç­¾åˆ—å’ŒIDåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    drop_cols = [label_col]
    if id_col in df.columns:
        drop_cols.append(id_col)

    feature_cols = [c for c in df.columns if c not in drop_cols]
    X = df[feature_cols].values.astype(np.float32)
    y = df[label_col].values.astype(int)

    print(f"ç‰¹å¾æ•°é‡ï¼š {X.shape[1]}")
    print("æ ‡ç­¾åˆ†å¸ƒï¼š")
    print(pd.Series(y).value_counts())

    # è®­ç»ƒ / æµ‹è¯• åˆ’åˆ†
    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X, y,
        test_size=test_size,
        random_state=random_state,
        stratify=y,
    )

    # å†ä» trainval ä¸­åˆ‡å‡ºéªŒè¯é›†
    val_ratio = val_size / (1.0 - test_size)
    X_train, X_val, y_train, y_val = train_test_split(
        X_trainval, y_trainval,
        test_size=val_ratio,
        random_state=random_state,
        stratify=y_trainval,
    )

    print(
        f"è®­ç»ƒé›†å¤§å°ï¼š ({X_train.shape[0]}, {X_train.shape[1]})   "
        f"éªŒè¯é›†å¤§å°ï¼š ({X_val.shape[0]}, {X_val.shape[1]})   "
        f"æµ‹è¯•é›†å¤§å°ï¼š ({X_test.shape[0]}, {X_test.shape[1]})"
    )

    # è®¡ç®— pos_weightï¼ˆç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
    num_pos = (y_train == 1).sum()
    num_neg = (y_train == 0).sum()
    pos_weight_value = num_neg / max(num_pos, 1)
    print(f"è®­ç»ƒé›†ä¸­é˜³æ€§={num_pos}, é˜´æ€§={num_neg}, pos_weight={pos_weight_value:.2f}")

    # ç¼ºå¤±å€¼å¡«è¡¥ + æ ‡å‡†åŒ–ï¼ˆæ‹ŸåˆåªåŸºäºè®­ç»ƒé›†ï¼‰
    imputer = SimpleImputer(strategy="mean")
    scaler = StandardScaler()

    X_train = imputer.fit_transform(X_train)
    X_train = scaler.fit_transform(X_train)

    X_val = imputer.transform(X_val)
    X_val = scaler.transform(X_val)

    X_test = imputer.transform(X_test)
    X_test = scaler.transform(X_test)

    # è½¬æˆ float32
    X_train = X_train.astype(np.float32)
    X_val = X_val.astype(np.float32)
    X_test = X_test.astype(np.float32)

    pos_weight_tensor = torch.tensor(
        pos_weight_value, dtype=torch.float32, device=DEVICE
    )

    return (
        X_train,
        y_train,
        X_val,
        y_val,
        X_test,
        y_test,
        pos_weight_tensor,
    )


# ==================== Dataset å®šä¹‰ ====================

class ProteinDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)  # (N, num_features)
        self.y = torch.tensor(y, dtype=torch.float32)  # (N,)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        seq = self.X[idx]        # (seq_len,) è¿™é‡ŒæŠŠæ¯ä¸ªæ ·æœ¬è§†ä¸ºä¸€æ¡â€œåºåˆ—â€
        label = self.y[idx]      # æ ‡é‡
        return seq, label


# ==================== æ¨¡å‹å®šä¹‰ï¼šBiLSTM å’Œ BiGRU ====================

class BiLSTM(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3):
        """
        input_dim: æ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾ç»´åº¦ï¼Œè¿™é‡ŒæŠŠ 141 ä¸ªè›‹ç™½è§†ä¸ºé•¿åº¦=141 çš„åºåˆ—ï¼Œæ¯æ­¥1ç»´
        hidden_dim: LSTM éšå±‚å¤§å°
        num_layers: å †å çš„ LSTM å±‚æ•°
        """
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        # x: (batch, seq_len)  â†’  (batch, seq_len, 1)
        x = x.unsqueeze(-1)
        out, _ = self.lstm(x)           # (batch, seq_len, 2*hidden)
        last = out[:, -1, :]            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        logits = self.fc(last).squeeze(-1)  # (batch,)
        return logits


class BiGRU(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3):
        super().__init__()
        self.gru = nn.GRU(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0.0,
        )
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2, 64),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        x = x.unsqueeze(-1)             # (batch, seq_len, 1)
        out, _ = self.gru(x)            # (batch, seq_len, 2*hidden)
        last = out[:, -1, :]
        logits = self.fc(last).squeeze(-1)
        return logits


# ==================== è¯„ä¼°å‡½æ•° ====================

def evaluate(model, dataloader, name="Model", threshold=0.5, verbose=True):
    model.eval()
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)

            logits = model(X_batch)          # (batch,)
            probs = torch.sigmoid(logits)    # (batch,)

            all_probs.append(probs.cpu().numpy())
            all_labels.append(y_batch.cpu().numpy())

    y_prob = np.concatenate(all_probs, axis=0)
    y_true = np.concatenate(all_labels, axis=0)

    # ä¿é™©ï¼šæŠŠ NaN / Inf æ¸…æ‰
    y_prob = np.nan_to_num(y_prob, nan=0.5, posinf=1.0, neginf=0.0)

    # AUC éœ€è¦æ­£è´Ÿç±»éƒ½å­˜åœ¨
    if len(np.unique(y_true)) < 2:
        val_auc = 0.5
    else:
        val_auc = roc_auc_score(y_true, y_prob)

    y_pred = (y_prob >= threshold).astype(int)
    acc = accuracy_score(y_true, y_pred)

    if verbose:
        print(f"[{name}] AUC = {val_auc:.4f}, ACC = {acc:.4f}")
        print("æ··æ·†çŸ©é˜µï¼š")
        print(confusion_matrix(y_true, y_pred))
        print("åˆ†ç±»æŠ¥å‘Šï¼š")
        print(classification_report(y_true, y_pred, digits=4))

    return val_auc, acc, y_true, y_prob, y_pred


# ==================== è®­ç»ƒå‡½æ•° ====================

def train_model(
    model,
    train_loader,
    val_loader,
    pos_weight,
    num_epochs=EPOCHS,
    model_name="Model",
):
    """æ ‡å‡†è®­ç»ƒå¾ªç¯ + éªŒè¯é›† AUC æ—©åœã€‚"""
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)

    best_auc = -1.0
    best_state_dict = None
    no_improve_epochs = 0

    for epoch in range(1, num_epochs + 1):
        model.train()
        running_loss = 0.0

        for X_batch, y_batch in train_loader:
            X_batch = X_batch.to(DEVICE)
            y_batch = y_batch.to(DEVICE)

            optimizer.zero_grad()
            logits = model(X_batch)       # (batch,)
            loss = criterion(logits, y_batch)

            if torch.isnan(loss):
                print(f"[{model_name}] ç¬¬ {epoch} ä¸ª epoch å‡ºç° NaN lossï¼Œè·³è¿‡è¯¥ batch")
                continue

            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            running_loss += loss.item() * X_batch.size(0)

        avg_loss = running_loss / len(train_loader.dataset)

        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_auc, val_acc, _, _, _ = evaluate(
            model, val_loader, name=model_name, verbose=False
        )

        print(
            f"[{model_name}] Epoch [{epoch}/{num_epochs}] "
            f"Train Loss = {avg_loss:.4f} | Val AUC = {val_auc:.4f} | Val ACC = {val_acc:.4f}"
        )

        # æ—©åœé€»è¾‘ï¼šVal AUC æå‡å°±ä¿å­˜
        if val_auc > best_auc:
            best_auc = val_auc
            best_state_dict = model.state_dict()
            no_improve_epochs = 0
            print(f"  ğŸ”¥ [{model_name}] Val AUC æå‡ä¸º {val_auc:.4f}ï¼Œä¿å­˜å½“å‰æ¨¡å‹")
        else:
            no_improve_epochs += 1
            if no_improve_epochs >= EARLY_STOP_PATIENCE:
                print(
                    f"  â—[{model_name}] Val AUC è¿ç»­ {EARLY_STOP_PATIENCE} ä¸ª epoch æœªæå‡ï¼Œæå‰åœæ­¢"
                )
                break

    if best_state_dict is not None:
        model.load_state_dict(best_state_dict)

    print(f"\n================= {model_name} åœ¨éªŒè¯é›†ä¸Šçš„æœ€ç»ˆè¡¨ç° =================")
    evaluate(model, val_loader, name=model_name + " Final Val", verbose=True)

    return model, best_auc


# ==================== ä¸»å‡½æ•° ====================

def main():
    # 1) åŠ è½½æ•°æ®
    (
        X_train,
        y_train,
        X_val,
        y_val,
        X_test,
        y_test,
        pos_weight,
    ) = load_and_split_data(DATA_PATH)

    # 2) æ„é€  Dataset / DataLoader
    train_dataset = ProteinDataset(X_train, y_train)
    val_dataset = ProteinDataset(X_val, y_val)
    test_dataset = ProteinDataset(X_test, y_test)

    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        drop_last=False,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        drop_last=False,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        drop_last=False,
    )

    # 3) è®­ç»ƒ BiLSTM
    bilstm = BiLSTM(input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3).to(DEVICE)
    print(bilstm)
    print("\n================= å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š BiLSTM =================")
    bilstm, best_auc_lstm = train_model(
        bilstm, train_loader, val_loader, pos_weight, num_epochs=EPOCHS, model_name="BiLSTM"
    )

    print("\n================= BiLSTM åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° =================")
    _, _, y_test_true, y_test_prob_lstm, _ = evaluate(
        bilstm, test_loader, name="BiLSTM Test", verbose=True
    )

    # 4) è®­ç»ƒ BiGRU
    bigru = BiGRU(input_dim=1, hidden_dim=64, num_layers=1, dropout=0.3).to(DEVICE)
    print(bigru)
    print("\n================= å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š BiGRU =================")
    bigru, best_auc_gru = train_model(
        bigru, train_loader, val_loader, pos_weight, num_epochs=EPOCHS, model_name="BiGRU"
    )

    print("\n================= BiGRU åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç° =================")
    _, _, _, y_test_prob_gru, _ = evaluate(
        bigru, test_loader, name="BiGRU Test", verbose=True
    )

    # 5) ç®€å•é›†æˆï¼šBiLSTM + BiGRU æ¦‚ç‡å¹³å‡
    print("\n================= å¤šæ¨¡å‹é›†æˆï¼ˆBiLSTM + BiGRUï¼Œç­‰æƒæ¦‚ç‡å¹³å‡ï¼‰ =================")
    ens_probs_eq = (y_test_prob_lstm + y_test_prob_gru) / 2.0
    ens_probs_eq = np.nan_to_num(ens_probs_eq, nan=0.5, posinf=1.0, neginf=0.0)

    if len(np.unique(y_test_true)) < 2:
        ens_auc = 0.5
    else:
        ens_auc = roc_auc_score(y_test_true, ens_probs_eq)

    ens_pred = (ens_probs_eq >= 0.5).astype(int)
    ens_acc = accuracy_score(y_test_true, ens_pred)

    print(f"Equal-Weighted Ensemble Test AUC = {ens_auc:.4f}, ACC = {ens_acc:.4f}")
    print("ç­‰æƒ Ensemble æ··æ·†çŸ©é˜µï¼š")
    print(confusion_matrix(y_test_true, ens_pred))
    print("ç­‰æƒ Ensemble åˆ†ç±»æŠ¥å‘Šï¼š")
    print(classification_report(y_test_true, ens_pred, digits=4))


if __name__ == "__main__":
    main()
